<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kabir Kumar - AI Safety Research & Community</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            max-width: 700px;
            margin: 0 auto;
            padding: 2rem;
            color: #2c3e50;
        }
        nav {
            display: flex;
            gap: 2rem;
            margin-bottom: 3rem;
        }
        nav a {
            color: #2c3e50;
            text-decoration: none;
            font-weight: 500;
        }
        nav a:hover {
            color: #3498db;
        }
        h1 {
            font-size: 2rem;
            margin-bottom: 2rem;
        }
        h2 {
            font-size: 1.5rem;
            margin-top: 3rem;
            margin-bottom: 1rem;
            color: #34495e;
        }
        p {
            margin-bottom: 1.5rem;
        }
        ul {
            list-style-type: none;
            padding: 0;
        }
        li {
            margin-bottom: 0.5rem;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        .contact {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid #eee;
        }
    </style>
</head>
<body>
    <nav>
        <a href="#work">Work</a>
        <a href="#writing">Writing</a>
        <a href="#contact">Contact</a>
    </nav>

    <main>
        <p>Hey, I'm Kabir. I founded AI-Plans, where we're making AI alignment research more accessible and merit-based. I believe in the power of collective intelligence to solve hard problems, which is why I organize events that bring together diverse perspectives - from AI researchers to legal experts - to tackle AI safety challenges.</p>

        <p>I'm currently focused on mechanistic interpretability, trying to understand what's happening inside neural networks. Along with my team, I've authored guides and frameworks that help others probe these systems, including a practical guide to finding features in sparse autoencoders and the widely-referenced "Broad Vulnerability Categories for AI Safety Plans" framework.</p>

        <p>I organize events that bridge different domains in AI safety: the AI & Liability Ideathon bringing together lawyers and AI researchers, Critique-a-Thons where researchers collaboratively evaluate alignment proposals, and hands-on AI Alignment Evals Hackathons. These events have attracted participants and judges from institutions like DeepMind and MIT.</p>

        <h2 id="work">Recent Work</h2>
        <ul>
            <li>AI & Liability Ideathon (December 2024)</li>
            <li>AI Alignment Evals Hackathon (January 2025)</li>
            <li>Guide: Finding Features in Neural Networks with SAEs</li>
            <li>Framework: Broad Vulnerability Categories for AI Safety Plans</li>
        </ul>

        <h2 id="writing">Writing & Research</h2>
        <ul>
            <li>How to find a feature in a model with a premade sparse autoencoder</li>
            <li>Broad Vulnerability Categories for AI Safety Plans</li>
        </ul>

        <h2 id="community">Community</h2>
        <ul>
            <li>Discord: AI-Plans Community</li>
            <li>Events: Critique-a-Thons</li>
            <li>Speaking: Universities and AI Safety conferences</li>
        </ul>

        <div class="contact" id="contact">
            <p>Want to chat about AI safety, mechanistic interpretability, or organizing technical events? Reach out at <a href="mailto:kabir@ai-plans.com">kabir@ai-plans.com</a> or find me on Discord where I run several AI safety communities.</p>
        </div>
    </main>
</body>
</html>
